{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/fr/thumb/1/1d/Logo_T%C3%A9l%C3%A9com_SudParis.svg/1014px-Logo_T%C3%A9l%C3%A9com_SudParis.svg.png\" width=\"30%\"></img>\n",
    "</center>\n",
    "\n",
    "<center> <h2> NET 4103/7431 Complex Network </h2> </center>\n",
    "\n",
    "<center> <h3> Vincent Gauthier (vincent.gauthier@telecom-sudparis.eu) </h3> </center>\n",
    "\n",
    "### Note\n",
    "Avant de commencer les exercices, assurez-vous que tout fonctionne comme prévu. Tout d'abord, le redémarrage du kernel **(dans la barre de menus, sélectionnez le kernel $\\rightarrow$ Restart)**.\n",
    "\n",
    "Assurez-vous que vous remplir les célluler aux endroits marquer «YOUR CODE HERE». \n",
    "\n",
    "Veuillez supprimer les ligne «raise NotImplementedError()» dans toutes les cellules auxquelles vous avez répondu, ainsi que votre nom et prénom ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "NOM = \"XXX\"\n",
    "PRENOM = \"XXX\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Lab #2: PageRank</h3> \n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<img src=\"../../images/webmap.jpg\"></img>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Before 1998, the web graph was a largely unused source of information and ignored by search engines such as Yahoo, or Astalavista. However, researchers such as Jon Kleinberg [2] and Brin and Page [3] have used this resource to create an algorithmically elegant methodology that has helped develop the search engines we know today. They then mainly used the following idea: a hyperlink link from my web page to another page can be interpreted as a recommendation from me to the web page in question. The underlying idea that is exploited is the following: a web page that is quoted very often must have a better ranking than a less frequently cited web page. However, this idea is not new, but used as the sole selective criterion is not enough. For example, a letter of recommendation for a job from the CEO of Orange will surely carry more weight than the other 10 letters of recommendation that you could have received from random people. In conclusion, the importance of the recommendations (and not only their numbers) must also be taken into account in calculating the reputation of a web page. This is exactly the principle of **PageRank**.\n",
    "\n",
    "In current search engines, each indexed webpage is associated with a \"**PageRank**\" value. When you perform a search, the search engine returns the web pages corresponding to the keywords sorted in ascending order of their value of \"PageRank\". The underlying assumption that is used is that: the web page with the highest \"PageRank\" value must be the most relevant for the keywords considered.\n",
    "\n",
    "In short, the notoriety (PageRank value) of a web page is defined as follows: the reputation of a web page is important, if it is itself pointed by other web pages with significant notoriety. This circular reasoning is at the heart of the algorithm that will be developed during this exercise. Through this circular reasoning, we deduce that the values of PageRank are in fact the values of the stationary states of an immense Markov chain. The transitions of this Markov chain are defined by the web graph. The matrix of transitions of this Markov chain is called \"Google\" matrix $\\mathbf{G}$. In order to calculate this stationary state vector, however, the Markov chain in question must have a unique solution. For this, one of the conditions is that the transitions graph must be irreducible (ie the graph forms a connected component), which is not the case for the web graph (cf. Fig. 1). The PageRank algorithm allowed Brin and Page to intelligently bypass this difficulty applying a transformation on the adjacency matrix in order to assume the irreducibility of this one.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../../images/Scc.png\" width=\"500px\"></img>\n",
    "<div align=\"center\"><b>Fig. 1</b></div>\n",
    "</div>\n",
    "\n",
    "**PageRank's Algorithm**\n",
    "1. Normalization of the adjacency matrix of the web graph to generate a stochastic matrix\n",
    "2. we update the matrix from the previous step in order to take into account the Dangling nodes (the node that don't have any outgoing link)\n",
    "3. Create the \"Google\" Matrix\n",
    "4. Compute the stationary state of the Markov chain\n",
    "\n",
    "In the first part of this exiercice we are going to considere the follwing directed graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ (cf. **Fig. 2**), as small example of the web graph. Each edge $e \\in \\mathcal{E}$ represente one hyperlink from one webpage $v \\in \\mathcal{V}$ to another.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../../images/GraphPageRank.png\" width=\"400px\"></img>\n",
    "<div align=\"center\"><b>Fig. 2</b></div>\n",
    "</div>\n",
    "\n",
    "### Notations\n",
    "$\\mathbf{A}$: The adjacency matrix of the graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$\n",
    "\n",
    "$\\mathbf{H}$: Normalized adjacency matrix\n",
    "\n",
    "$\\mathbf{G}$: The dense, stochastic matrix called teh \"Google\" matrix\n",
    "\n",
    "$\\mathbf{\\pi}^T$: PageRank Vector, a stationnay vector of the Markov chain describe by the matrix $\\mathbf{G}$\n",
    "\n",
    "$n$ : Number of the web page (in our graph)\n",
    "\n",
    "$\\alpha$: parameter between 0 and 1\n",
    "\n",
    "### Conventions\n",
    "\n",
    "$\\mathbf{e}^T$ : is vector line $\\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}$\n",
    "\n",
    "$\\mathbf{e}$ : is a vector colomn  $\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$\n",
    "\n",
    "$\\sigma(\\mathbf{A})$ est l'ensemble (set) des valeurs propres de la matrice $\\mathbf{A}$\n",
    "\n",
    "$\\lambda_{max} = \\max_{\\lambda \\in \\sigma(\\mathbf{A})} \\lvert \\lambda \\lvert$ est la plus grande valeur propre de la matrice $\\mathbf{A}$\n",
    "\n",
    "$\\mathbf{e} \\otimes \\mathbf{e}^T$ : est le produit exterieur $\\begin{pmatrix} 1 \\\\ 2 \\\\ 3  \\end{pmatrix}  \\otimes \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 & 3  \\\\ 2 & 4 & 6 \\\\ 3 & 6 & 9 \\end{pmatrix}$\n",
    "\n",
    "### Python Reminder \n",
    "Multiplication and division with Numpy:\n",
    "```Python\n",
    "import numpy as np\n",
    "x = np.array([2, 2, 2])\n",
    "2*x\n",
    ">>> array([4, 4, 4])\n",
    "x = np.array([2, 2, 2])\n",
    "x/2\n",
    ">>> array([1, 1, 1])\n",
    "A = np.array([[2, 2, 2], [2, 2, 2], [2, 2, 2]])\n",
    "A/2\n",
    ">>> np.array([[1, 1, 1], \n",
    "              [1, 1, 1], \n",
    "              [1, 1, 1]])\n",
    "```\n",
    "\n",
    "Matrix vector division:\n",
    "```Python\n",
    "A = np.array([[1, 1], [4, 4]])\n",
    "b = np.array([1, 3])\n",
    "(A.T/b).T\n",
    ">>> array([[ 1.        ,  1.        ],\n",
    "           [ 1.33333333,  1.33333333]])\n",
    "```\n",
    "\n",
    "Dot product:\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[2, 2, 2], [2, 2, 2]])\n",
    "x = np.array([2, 2, 2])\n",
    "np.dot(A,x)\n",
    ">>> array([12, 12])\n",
    "\n",
    "a = np.array([1, 1, 1])\n",
    "b = np.array([1, 1, 1])\n",
    "np.outer(a,b)\n",
    ">>> array([[1, 1, 1], \n",
    "           [1, 1, 1], \n",
    "           [1, 1, 1]])\n",
    "```\n",
    "\n",
    "Matrix transpose:\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "A.T\n",
    ">>> array([[1, 3],\n",
    "           [2, 4]])\n",
    "```\n",
    "\n",
    "Shape of a matrix\n",
    "```Python\n",
    "A = np.array([[1, 1, 1], [1, 1, 1]])\n",
    "n, m = A.shape\n",
    "print(n, m)\n",
    ">>> 2 3\n",
    "```\n",
    "\n",
    "Sum of each row entries of a matrix\n",
    "```Python\n",
    "A = np.array([[1, 1, 1,], [2, 2, 2]])\n",
    "A[1,:].sum()\n",
    ">>> 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Style pour le Notebook\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def css_styling():\n",
    "    styles = open(\"../../styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a synthetic web-graph with networkx\n",
    "\n",
    "With the help of networkx generate the directed graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ (cf. **Fig 2**), and draw the graph with matplotlib in order to validate visualy that you obtain the same graph as in the **figure 2**. \n",
    "\n",
    "**Help**: Use the already defined functions in the networkx library to add vertices and edges to the graph ([networkx tutorial](https://networkx.github.io/documentation/networkx-1.10/tutorial/tutorial.html#nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce5deac44e9dcdf79565f93d9e750dac",
     "grade": false,
     "grade_id": "cell-b38b9e2b9a5a33ff",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "g = nx.DiGraph()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "pos = nx.spring_layout(g);\n",
    "nx.draw_networkx(g, pos=pos, node_size=600, font_size=20.0);\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff2b2087ce0fad434bdc059c5fc2b5c3",
     "grade": true,
     "grade_id": "cell-eb40bde129593b4e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(g.nodes()) == 6\n",
    "assert g.has_edge(6,4)\n",
    "assert g.has_edge(4,6)\n",
    "assert g.has_edge(2,5) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the adjacency matrix of a graph \n",
    "\n",
    "Before going further in the exercise, let's extract the adjacency matrix $\\mathbf{A}_{n \\times n}$ from the directed graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ to construct the normalized adjacency matrix of the $\\mathbf{H}_{n \\times n} $ graph.\n",
    "\n",
    "(eq. **1**)\n",
    "$$\n",
    "\\mathbf{H}_{ij} = \\begin{cases}\n",
    "    1/\\sum_{j} \\mathbf{A}_{ij} & \\text{ if there is a link between node } i \\text{ and } j\\\\\n",
    "    0 & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Exemple**: The matrix $\\mathbf{H}_{n \\times n}$ of the graph in figure 2.\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\begin{pmatrix}\n",
    "  0 & 1/2 & 1/2 & 0 & 0 & 0 \\\\\n",
    "  0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "  1/3 & 1/3 & 0 & 0 & 1/3 & 0 \\\\\n",
    "  0 & 0 & 0 & 0 & 1/2 & 1/2 \\\\\n",
    "  0 & 0 & 0 & 1/2 & 0 & 1/2 \\\\\n",
    "  0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Example: How to export the adjacency matrix from a networkx graph**\n",
    "```Python\n",
    ">>> import numpy as np\n",
    ">>> G = nx.Graph([(1,1)])\n",
    ">>> A = nx.to_numpy_matrix(G)\n",
    ">>> A\n",
    "matrix([[ 1.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33deec33a20f0cd30da411e804e74047",
     "grade": false,
     "grade_id": "cell-a559fbd8ae7bdf62",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def transform_to_stochatic(G):\n",
    "    ### Ignore division by zero warmings\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9f0cd5f0a94b485832df4dc0c189f53",
     "grade": true,
     "grade_id": "cell-b603cb0d07644bd0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "H = transform_to_stochatic(g)\n",
    "\n",
    "H_res = np.array( \n",
    "[[ 0.,          0.5,         0.5,         0.,          0.,          0.,        ],\n",
    " [ 0.,          0.,          0.,          0.,          0.,          0.,        ],\n",
    " [ 0.33333333,  0.33333333,  0.,          0.,          0.33333333,  0.,        ],\n",
    " [ 0.,          0.,          0.,          0.,          0.5,         0.5,       ],\n",
    " [ 0.,          0.,          0.,          0.5,         0.,          0.5,       ],\n",
    " [ 0.,          0.,          0.,          1.,          0.,          0.,        ]])\n",
    "\n",
    "np.testing.assert_allclose(H, H_res, rtol=1e-4)\n",
    "assert isinstance(H, np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding dangling nodes \n",
    "\n",
    "We want to calculate the \"dangling vector\" $\\mathbf{a}$ which associates with each node the value 1 if this one does not have any outgoing link, and 0 otherwise.\n",
    "\n",
    "(eq. **2**)\n",
    "\n",
    "$$\n",
    "\\mathbf{a}_i = \\begin{cases}\n",
    "    1 & si \\sum_{j} \\mathbf{A}_{ij} = 0 \\\\\n",
    "    0              & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Example**:\n",
    "<img src=\"../../images/DanglingPageRank.png\" width=\"500px\"></img>\n",
    "<div align=\"center\"><b>Fig. 3</b>: the correponding vector $\\mathbf{a}$ of the graph defined in figure 2.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c34e179bdc0905e6995ebbeac203b9d5",
     "grade": false,
     "grade_id": "cell-16160fe83dc86883",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dangling_vector(H):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8e49c909b22b7fc7d393f42ec49e7b4",
     "grade": true,
     "grade_id": "cell-4c9063860247f96f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "a = dangling_vector(H)\n",
    "np.testing.assert_array_equal(a, [ 0.,  1.,  0.,  0.,  0.,  0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the \"Google\" Matrix \n",
    "\n",
    "Unfortunately, the graph of the web does not form a connected graph, which is neither irreducible nor aperiodic. By constructing the \"Google\" matrix, we will add artificial transitions to the adjacency matrix of the web graph in order to transform it into another graph that offers satisfactory properties (irreducible and aperiodic). Once the $ \\mathbf{G}_{n \\times n}$ matrix has been built, it will be used to calculate the steady states $\\pi^T$ (the PageRank) of each node of the graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$. The goal of this exercise is to transform the $\\mathbf{A}_{n \\times n}$ adjacency matrix of the web graph into a matrix $\\mathbf{G}_{n \\times n}$ so that it satisfies the following conditions:\n",
    "\n",
    "1. The matrix $\\mathbf{G}$ should be stochastic. \n",
    "2. The matrix $\\mathbf{G}$ should be irreductible.\n",
    "3. The matrix $\\mathbf{G}$ should be aperiodic.\n",
    "4. The matrix $\\mathbf{G}$ should be primitive. \n",
    "\n",
    "<div class=warn>\n",
    "<b>Definition:</b> A matrix is called primitive if it admits a power whose terms are strictly positive (i.e., there is $ \\mathbf{G}^k > 0 $).\n",
    "</div>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div class=warn>\n",
    "<b>Theorem:</b> if the matrix $\\mathbf{G}$ is primitive the it exist a vector $\\pi^{(k)T}$ such that $\\mathbf{\\pi}^{(k)T} = \\mathbf{\\pi}^{(k)T} \\mathbf{G}$. In other words, there is convergence of the vector $\\pi^{(k)}$ to a single stationary state when $k \\to \\infty$, see Perron-Frobenius theorem.\n",
    "</div>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div class=warn>\n",
    "<b>Theorem of Perron-Frobenius:</b> Let $ \\mathbf{A} $ be a positive and primitive matrix. Then there exist a value $ \\lambda_{max}$ such that:\n",
    "\n",
    "1. $\\lambda_{max}$ is a real positive value, $\\lambda_{max} > 0$ <br>\n",
    "2. $\\lambda_{max}$ is associated to eigenvectors strictly positive <br>\n",
    "3. $\\lambda_{max} > \\lvert \\lambda \\lvert\\ \\ \\forall \\lambda \\neq \\lambda_{max}$ <br>\n",
    "4. it exist a unique vector $\\mathbf{x}$ (with $\\lvert\\lvert \\mathbf{x} \\lvert\\lvert_{1} = 1$) such that  $\\mathbf{A}\\mathbf{x}=\\lambda_{max} \\mathbf{x}$\n",
    "</div>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div class=green>\n",
    "<b>Conclusion:</b> the adjacency matrix $\\mathbf{A}_{n \\times n}$ of an ireductible and aperiodic graph is a primitve matrix. From the Perron-Frobenius theorem, we can conlude that the adjacency matrix $\\mathbf{A}_{n \\times n}$ has an unique vector $\\mathbf{x}$ where all entries are strictly positive and such as $\\mathbf{A}\\mathbf{x}=\\lambda_{max} \\mathbf{x}$.\n",
    "</div>\n",
    "\n",
    "\n",
    "#### We define\n",
    "\n",
    "(eq. **3**)\n",
    "$$\n",
    "\\mathbf{S} = \\mathbf{H} + \\frac{1}{n} \\mathbf{a} \\otimes \\mathbf{e}^T \\\\\n",
    "$$\n",
    "\n",
    "Remider, the vector $\\mathbf{a}_n$ (previously computed) is the dangling vector and $\\mathbf{H}_{n \\times n}$ is the stochastic matrix of the adjacency matrix of the graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ (previously computed). \n",
    "\n",
    "#### We define the \"Google matrix\" as follow: \n",
    "\n",
    "(eq. **4**)\n",
    "$$\n",
    "\\mathbf{G} = \\alpha  \\mathbf{S} + (1-\\alpha) \\frac{1}{n}\\mathbf{e} \\otimes \\mathbf{e}^T \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../../images/GoogleMatrice.png\" width=\"700px\"></img>\n",
    "<div align=\"center\"><b>Fig. 4</b>: Google matrice</div>\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "The \"Google matrice\" $\\mathbf{G}_{n \\times n} $ (cf. eq. **4**) is build by adding complementary transitions such as in figure **3** to the original stochastic matric of the graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ in order to satisfy the conditions 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Expand equation 4 (using equation 3) to express the $\\mathbf{G}_{n \\times n}$  as a function of $ \\mathbf{H}_{n \\times n} $ and $ \\mathbf{a}_n $\n",
    "\n",
    "\n",
    "**Response**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the function that define the \"Google matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aee6d7704346ed61d4a0e3faada49f85",
     "grade": false,
     "grade_id": "cell-a6428e1d936486ec",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def google_matrix(H, a, α=0.9):\n",
    "    '''\n",
    "    H: stochastic adjacency matrix\n",
    "    a : dangling vector\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3911ba920c7cef96cde7b015ad3cc842",
     "grade": true,
     "grade_id": "cell-879580d18840fc1e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "G = google_matrix(H, a)\n",
    "print(G)\n",
    "\n",
    "G_res = np.array(\n",
    "[[ 0.01666667,  0.46666667,  0.46666667,  0.01666667,  0.01666667,  0.01666667],\n",
    " [ 0.16666667,  0.16666667,  0.16666667,  0.16666667,  0.16666667,  0.16666667],\n",
    " [ 0.31666667,  0.31666667,  0.01666667,  0.01666667,  0.31666667,  0.01666667],\n",
    " [ 0.01666667,  0.01666667,  0.01666667,  0.01666667,  0.46666667,  0.46666667],\n",
    " [ 0.01666667,  0.01666667,  0.01666667,  0.46666667,  0.01666667,  0.46666667],\n",
    " [ 0.01666667,  0.01666667,  0.01666667,  0.91666667,  0.01666667,  0.01666667]])\n",
    "\n",
    "np.testing.assert_allclose(G, G_res, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numercial application based on the graph depicted in the figure 2  with $\\alpha = 0.9$\n",
    "\n",
    "(**eq. 4.**)\n",
    "$$\n",
    "\\mathbf{G} = 0.9\\mathbf{H} + \\left[ 0.9 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{pmatrix} + 0.1  \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ \\end{pmatrix}  \\right]  \\otimes 1/6  \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\end{pmatrix} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{G} =  \\begin{pmatrix} \n",
    "1/60 & 7/15 & 7/15 & 1/60 & 1/60 & 1/60 \\\\\n",
    "1/6  & 1/6  & 1/6  & 1/6  & 1/6  & 1/6 \\\\\n",
    "19/60 & 19/60 & 1/60 & 1/60 & 19/60 & 1/60 \\\\\n",
    "1/60 & 1/60 & 1/60 & 1/60 & 7/15 & 7/15 \\\\\n",
    "1/60 & 1/60 & 1/60 & 7/15 & 1/60 & 7/15 \\\\\n",
    "1/60 & 1/60 & 1/60 & 11/12 & 1/60 & 1/60 \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the PageRank vector\n",
    "\n",
    "(**eq. 5**)\n",
    "\\begin{eqnarray}\n",
    "& \\mathbf{\\pi}^{(k)T} & =   \\mathbf{\\pi}^{(k-1)T} \\mathbf{G} \\\\ \n",
    "& \\mathbf{\\pi}^{(k-1)T} & = \\mathbf{\\pi}^{(k-2)T} \\mathbf{G} \\\\ \n",
    "& \\vdots &\\\\\n",
    "& \\mathbf{\\pi}^{(1)T} & = \\mathbf{\\pi}^{(0)T} \\mathbf{G} \n",
    "\\end{eqnarray}\n",
    "\n",
    "By induction we can therefore write\n",
    "\n",
    "(**eq. 6**)\n",
    "$$\\mathbf{\\pi}^{(k)T} = \\mathbf{\\pi}^{(0)T} \\mathbf{G}^k$$\n",
    "\n",
    "We now seek to compute the stationary vector $\\mathbf{\\pi}^{(k) T}$ when $k \\ to \\ infty$. We know that the matrix $ \\mathbf{G} $ is primitive by construction, then the converge of the vector $\\mathbf{\\pi}^{T}$ (in the case of a finite state space) is proved by the Perron-Frobenius theorem. In order to calculate the stationary states of the vector $\\mathbf{\\pi}^{T}$ we will then iterate the operation $\\mathbf{\\ pi}^{(k) T} = \\mathbf{\\pi}^{ (k-1) T} \\mathbf{G}$ until convergence of the vector $\\mathbf{\\pi}^{T}$ (cf. Alg 1).\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"../../images/PowerIt.png\" width=\"600px\"></img>\n",
    "\n",
    "<br>\n",
    "<div align=\"center\"><b>Alg1</b>: Power iteration</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af18f486d818338d49a047879d7d25b0",
     "grade": false,
     "grade_id": "cell-0f2de665f401ce93",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def power_iter(G, max_iter=500, tol=1e-6):\n",
    "    \"\"\"power_iter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : Goole matrix\n",
    "\n",
    "    max_iter : integer, optional\n",
    "        Maxium number of iteration\n",
    "\n",
    "    tol : float, optional\n",
    "       Error tolerance used to check convergence in power method solver.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    raise RuntimeError('pagerank: power iteration failed to converge in %d iterations.' % max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e55e1132cfcaa077d1d123b2a87a4da1",
     "grade": true,
     "grade_id": "cell-30e261076683cffa",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "(π, gap) = power_iter(G, max_iter=100, tol=1e-6)\n",
    "print(π)\n",
    "\n",
    "π_res = np.array([ 0.03721313,  0.05395937,  0.04150701,  0.37507848,  0.20599777,  0.28624425])\n",
    "\n",
    "np.testing.assert_allclose(π, π_res, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ, _ = LA.eig(G)\n",
    "λ_2 = np.sort(λ.real)[-2]\n",
    "λ_gap = [λ_2 ** i for i in range(1,25)]\n",
    "\n",
    "fig = plt.figure(figsize=(7,4))\n",
    "plt.semilogy(gap, 'ro')\n",
    "plt.semilogy(λ_gap, lw=2.0, label=\"$\\lambda_2^i$\")\n",
    "plt.title(\"Convergence rate of the PageRank algorithm\")\n",
    "plt.xlabel(\"Number of iteration $(i)$\")\n",
    "plt.ylabel(\"gap\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "print(π)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  Integrate the previously built functions in order to finalize PageRank algorithm\n",
    "\n",
    "Implement the function that integrates the different steps that make up the PageRank algorithm. You should use the functions you have already defined earlier to build the final algorithm.\n",
    "\n",
    "#### Reminder of step need to build the PageRank algorithm\n",
    "1. Normalization of the matrix of ajacence of the graph of the web\n",
    "2. Find the \"Dangling nodes\" (the nodes that don't have any outgoing links)\n",
    "3. Creation of the \"Google\" Matrix\n",
    "4. Calculating the stationary state vector of the Markov chain $\\mathbf{G}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f93633bb468821a384119beb14e0ba3f",
     "grade": false,
     "grade_id": "cell-4db00d895b96dc80",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def pagerank(Graph, α=0.85, max_iter=100, tol=1e-6):\n",
    "    \"\"\"Return the PageRank of the nodes in the graph.\n",
    "\n",
    "    PageRank computes a ranking of the nodes in the graph G based on\n",
    "    the structure of the incoming links. It was originally designed as\n",
    "    an algorithm to rank web pages.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    G : graph\n",
    "      A NetworkX graph.  Undirected graphs will be converted to a directed\n",
    "      graph with two directed edges for each undirected edge.\n",
    "\n",
    "    alpha : float, optional\n",
    "      Damping parameter for PageRank, default=0.85.\n",
    "\n",
    "    max_iter : integer, optional\n",
    "      Maximum number of iterations in power method.\n",
    "\n",
    "    tol : float, optional\n",
    "      Error tolerance used to check convergence in power method solver.\n",
    "      \n",
    "    Return\n",
    "    PR : dict\n",
    "      retourne un dictionnaire avec comme clé le nodeid, et comme valeur le pagerank du noeud \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e130eb2b8901bc945487baa044fc572c",
     "grade": true,
     "grade_id": "cell-0fb3eb924a1dcb99",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "PR = pagerank(g)\n",
    "print(PR)\n",
    "\n",
    "PR_res = dict({\n",
    "    1: 0.05170556259095016, \n",
    "    2: 0.07368068204240269, \n",
    "    3: 0.057413363969125462, \n",
    "    4: 0.3487020460725242, \n",
    "    5: 0.1999034157779406, \n",
    "    6: 0.2685949295470571})\n",
    "\n",
    "for k,v in PR_res.items():\n",
    "    np.testing.assert_allclose(PR[k], PR_res[k], rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of the PageRank: ranking wikipedia article\n",
    "\n",
    "## Load the web graph of Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the wikipedia graph (networkx)\n",
    "Gwikipedia = nx.read_graphml(\"../../Data/Wikipedia/wikipedia.graphml\")\n",
    "\n",
    "# load the metadat into a pandas dataframe\n",
    "wikipedia_db = pd.read_pickle(\"../../Data/Wikipedia/wikipedia.pkl\")\n",
    "# wikipedia_db = pd.read_parquet(\"../../Data/Wikipedia/wikipedia.parquet.gzip\")\n",
    "# wikipedia_db = pd.read_csv(\"../../Data/Wikipedia/wikipedia.csv\", index_col=\"PageID\")\n",
    "\n",
    "wikipedia_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Calculate statistical properties of the graph of wikipedia\n",
    "\n",
    "Compute the following:\n",
    "1. the graph density. Reminder, for a graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ the density $D =\\vert \\mathcal{E}\\vert/N(N-1)$, \n",
    "2. the degree distribution of the node degree of the wikipedia graph\n",
    "3. the complementary cummulative distribution of the node degree of the wikipedia graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61fdbe9a3820e98aec32645ce5f4c43d",
     "grade": true,
     "grade_id": "cell-35511977dcf07d47",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#question 1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Plot teh degree distribution and teh complementary cummulative degree distribution\n",
    "#\n",
    "degree = [v for k,v in dict(Gwikipedia.degree()).items()]\n",
    "distribution = [(elem, degree.count(elem)) for elem in sorted(set(degree))]\n",
    "k,pk = zip(*distribution)\n",
    "PDF = np.array(pk)/sum(pk)\n",
    "CCDF = 1-np.cumsum(PDF)\n",
    "\n",
    "\n",
    "#\n",
    "# Plots \n",
    "#\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.loglog(k, PDF, 'ro')\n",
    "ax1.set_xlabel(\"$k$ Degree\")\n",
    "ax1.set_ylabel(\"$P_k$\")\n",
    "ax1.set_title(\"PDF\")\n",
    "\n",
    "ax2.loglog(k, CCDF, 'ro')\n",
    "ax2.set_ylim(1e-4,1.1)\n",
    "ax2.set_xlim(1,2e3)\n",
    "ax2.set_xlabel(\"$k$ Degree\")\n",
    "ax2.set_ylabel(\"$1-P[K > k]$\")\n",
    "ax2.set_title(\"CCDF\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "- Is the wikipedia graph dense ? or sparce ?\n",
    "- What is to say about the degree distribution of the wikipedia graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the PageRank of a subset of the wikipedia articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4b2a85a00dd343427b53721d2ebd78d",
     "grade": true,
     "grade_id": "cell-2abb4f66eee73d43",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# compute the pagerank with your previously defined functions\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "PR = [[k,v] for k,v in PR.items()]\n",
    "PR = pd.DataFrame(PR, columns=['PageID', 'PageRank'])\n",
    "PR = PR.set_index('PageID')\n",
    "PR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We assign a PageRank to each Wikipedia article \n",
    "wikipedia_db = wikipedia_db.join(PR)\n",
    "# We sort the wikipedia article acording to their PageRank\n",
    "wikipedia_db = wikipedia_db.sort_values(['PageRank'], ascending=0)\n",
    "# we print the top ranked wikipedia article \n",
    "wikipedia_db[[\"Page Title\", \"PageRank\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Wikipedia article with to keywords\n",
    "\n",
    "**Warning**: the number of wikipedia pages indexed in this database is of the order of 5000 articles only, written in English. The number of articles and therefore of keywords are therefore limited. Perform queries with keywords written in lowercase letters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyworkds\n",
    "mot_clee_1 = \"france\"\n",
    "mot_clee_2 = \"germany\"\n",
    "# Request\n",
    "wikipedia_db[(wikipedia_db['Keywords'].str.contains(mot_clee_1)==True) & (wikipedia_db['Keywords'].str.contains(mot_clee_2)==True)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords\n",
    "mot_clee_1 = \"france\"\n",
    "# Requete\n",
    "wikipedia_db[(wikipedia_db['Keywords'].str.contains(mot_clee_1)==True) ].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "Carry out jointly: a query on the database and with the google search engines:\n",
    "\n",
    "* Make a google Request \"site: wikipedia.org keyword1 keyword2\"\n",
    "* Qualitatively compare the requests made on Google and those made on the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation du PageRank avec un algorithme distribué\n",
    "\n",
    "\n",
    "The ranking of web pages through the \"PageRank\" algorithm is still considered to be the biggest matrix problem known to date. To give you an order of magnitude, in 2007 the \"power iteration\" which allowed the calculation of PageRank at Google occupied a data center for 15 days to finalize the calculation. In order to reduce the complexity of the computation $\\pi^T \\mathbf{G}$ we would like to perform a computation on a sparse matrix instead of the dense matrix $\\mathbf{G}$. At the same time, one wishes to be able to parallelize computation (multiprocessor, cluster of computation). An elegant way to obtain these two properties is to perform the following operation on all the nodes of the graph (Cf. eq. 7., Fig. 5, Algo. 3):\n",
    "<br>\n",
    "(**eq. 7**)\n",
    "$$ PR_{i} = \\frac{(1-\\alpha)}{n} + \\alpha \\sum_{j \\in \\mathcal{N}(i)} \\frac{PR_j}{L_j}$$\n",
    "<br>\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "<img src=\"../../images/PageRankDistributed2.png\" width=\"400px\"></img>\n",
    "<br />\n",
    "<div align=\"center\"><b>Fig. 5</b>: PageRank</div>\n",
    "</div>\n",
    "\n",
    "The operation described in equation 7. can be computed independently (in parallel) for all the nodes of the graph. This is the approach that is developed in software for calculating large volumes of data such as: \n",
    "\n",
    "* [Spark/Graphx](http://spark.apache.org/)\n",
    "\n",
    "or more simply with certain graphs' libraries of parallel computation:\n",
    "\n",
    "* [boost graph](http://www.boost.org/doc/libs/1_46_0/libs/graph_parallel/doc/html/page_rank.html)\n",
    "* [graph-tool](https://graph-tool.skewed.de/)\n",
    "<br>\n",
    "<br>\n",
    "<div align=\"center\">\n",
    "<img src=\"../../images/PageRankDistributed.png\" width=\"500px\"></img>\n",
    "<br />\n",
    "<div align=\"center\"><b>Alg 3</b>: PageRank distributed</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5875793d6c3049551e52f6076799aed6",
     "grade": false,
     "grade_id": "cell-2143b85bf96cee6b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def pagerank_distributed(g, α=0.9, max_iter=200, tol=1e-6):\n",
    "    N = len(g.nodes())\n",
    "    gc = g.copy()\n",
    "    \n",
    "    # on modifie le graphe pour ajouter les transitions au dangling node  \n",
    "    for dangling in g.nodes():\n",
    "        if gc.out_degree(dangling) == 0:\n",
    "            for n in g.nodes():\n",
    "                gc.add_edge(dangling, n)\n",
    "\n",
    "    # on initialise le pagerank a chaque noeud du graphe\n",
    "    for node in gc.nodes():\n",
    "        gc.node[node]['PageRankOld'] = 1.0/N\n",
    "    \n",
    "    # Power iteration\n",
    "    for _ in range(max_iter):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        if tol > N*diff:\n",
    "            return np.array([gc.node[n]['PageRank'] for n in gc.nodes()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67530456711dbd224a16f2e67ae9a4e6",
     "grade": true,
     "grade_id": "cell-78c2b295e77e9df0",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "PR = pagerank_distributed(g)\n",
    "print(PR)\n",
    "\n",
    "PR_res = np.array([\n",
    "    0.0372119873811025, \n",
    "    0.05395738811699192, \n",
    "    0.04150567933532273, \n",
    "    0.37508077029418346, \n",
    "    0.20599832111968525, \n",
    "    0.28624585375271416])\n",
    "\n",
    "np.testing.assert_allclose(PR, PR_res, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Amy N. Langville, Carl D. Meyer, \"[Deeper Inside PageRank](http://projecteuclid.org/euclid.im/1109190965)\", Internet Math., Vol. 1(3), pp. 335--380, 2003.\n",
    "\n",
    "[2] Jon Kleinberg, \"[Authoritative sources in a hyperlinked environment](http://www.cs.cornell.edu/home/kleinber/auth.pdf)\",  Journal of the ACM, 46(5), pp. 604–632, 1999.\n",
    "\n",
    "[3] L. Page, S. Brin, R. Motwani, T. Winograd, \"[The PageRank citation ranking: Bringing order to the Web](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf)\", published as a technical report on January 29, 1998."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
