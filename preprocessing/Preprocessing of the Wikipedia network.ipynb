{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auteur: Vincent Gauthier\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import collections\n",
    "import csv\n",
    "import urllib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vgauthier/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction d'extraction simple de keywords dans du text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text, limit=10):\n",
    "    keywords = []\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    text = nltk.Text(tokens)\n",
    "    # Filtre le text pour ne garder que les mots\n",
    "    words = [w.lower() for w in text if w.isalpha()]\n",
    "    # Filtre le text pour ne garder que les mots importants, on supprime 'of, the, ..etc'\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in words if w.lower() not in stopwords]\n",
    "    # on calcul la frequence de chaque mot clé\n",
    "    freq = FreqDist(content)\n",
    "    sorted_freq = collections.OrderedDict(sorted(freq.items(), key=lambda x: x[1], reverse=True))\n",
    "    num_keywords = 0\n",
    "    for k,v in sorted_freq.items():\n",
    "        keywords.append(k)\n",
    "        num_keywords += 1\n",
    "        if num_keywords >= limit:\n",
    "            return \", \".join(keywords)\n",
    "    return \", \".join(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test keyworks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese, zheng, china, map, ming, emperor, ships, voyages, muslim, malacca\n"
     ]
    }
   ],
   "source": [
    "with open(\"../Data/Wikipedia/plaintext_articles/Zheng_He.txt\", 'r') as textfile:\n",
    "    text = textfile.read()\n",
    "    print(get_keywords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse wikipedia dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import parse\n",
    "\n",
    "with open(\"../Data/wikipedia/articles.tsv\") as articlefile:\n",
    "    wikipedia = []\n",
    "    wiki_id = {}\n",
    "    idp = 1\n",
    "    reader = csv.reader(articlefile)\n",
    "    for row in reader:\n",
    "        if row and row[0][0] != \"#\":\n",
    "            with open(\"../Data/wikipedia/plaintext_articles/\" + row[0] + \".txt\", 'r') as textfile:\n",
    "                text = textfile.read()\n",
    "                keywords = get_keywords(text)\n",
    "            wiki_article = [idp, parse.unquote(row[0]), row[0], keywords]\n",
    "            wiki_id[row[0]] = idp\n",
    "            idp += 1\n",
    "            wikipedia.append(wiki_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = pd.DataFrame(wikipedia, columns=['PageID', 'Page Title', 'Page Title Encoded', 'Keywords'])\n",
    "wikipedia = wikipedia.set_index(['PageID'])\n",
    "wikipedia.to_pickle(\"../Data/wikipedia/wikipedia.pkl\", protocol=4)\n",
    "wikipedia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.to_parquet('../Data/wikipedia/wikipedia.parquet.gzip', compression='gzip')\n",
    "wikipedia.to_csv('../Data/wikipedia/wikipedia.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Savegarde le graphe des leiens entre page wikipedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/wikipedia/links.tsv') as links:\n",
    "    G = nx.DiGraph()\n",
    "    for idx, row in wikipedia.iterrows():\n",
    "        G.add_node(idx)\n",
    "    reader = csv.reader(links, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        if row and row[0][0] != \"#\":\n",
    "            G.add_edge(wiki_id[row[0]], wiki_id[row[1]])\n",
    "            G.nodes[wiki_id[row[0]]]['title'] = parse.unquote(row[0])\n",
    "            G.nodes[wiki_id[row[1]]]['title'] = parse.unquote(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(G, \"../Data/wikipedia/wikipedia.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(G, \"../Data/wikipedia/wikipedia.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
